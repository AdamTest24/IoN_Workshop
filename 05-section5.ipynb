{
 "cells": [
  {
   "cell_type": "raw",
   "id": "507da0b0",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Neuro Images Clustering\"\n",
    "teaching: 10\n",
    "exercises: 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffcef75",
   "metadata": {},
   "source": [
    "[**Download Chapter notebook (ipynb)**](05-section5.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a13981",
   "metadata": {},
   "source": [
    "- What is NiBabel?\n",
    "- How NiBabel used to import neuro-images?\n",
    "- How is machine learning applied to image data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0d8fa",
   "metadata": {},
   "source": [
    "- Installing and using NiBabel Python Library\n",
    "- Understanding data pre-processing\n",
    "- Segmenting images with Gaussian Mixture Models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d8027",
   "metadata": {},
   "source": [
    "## NiBabel Image Library\n",
    "\n",
    "Firstly, if you haven't already installed NiBabel on your command line, please do so in this notebook by uncommenting and running the code in the cell below.\n",
    "\n",
    "```\n",
    "# conda install -c conda-forge nibabel\n",
    "```\n",
    "\n",
    "Let's begin by importing the nibabel package. Here we are importing it as `nib` - an abbreviation of our choosing to access the package's functions more easily in our code. You can use any abbreviation that you like; just remember to call it correctly in any subsequent code that you write.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42eae2b",
   "metadata": {},
   "source": [
    "## Note\n",
    "Note how we import the NiBabel package as `nib`. You can use any abbreviation to access the packageâ€™s functions from within your programme.\n",
    "\n",
    "To familiarise yourself with the NiBabel package, try the [Getting started tutorial](https://nipy.org/nibabel/gettingstarted.html) using an example image file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa85b5",
   "metadata": {},
   "source": [
    "### **Load images and get data**\n",
    "\n",
    "NiBabel has its own import function `load()`. Let's load in our image, and assign this to a variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_nibabel = nib.load(\"data/T1_mask.nii\")\n",
    "\n",
    "type(img_nibabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0f4db8",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "<class 'nibabel.nifti1.Nifti1Image'>\n",
    "```\n",
    "\n",
    "The `header` lets you see any metadata that is associated with the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024d15d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_info = img_nibabel.header\n",
    "\n",
    "print(meta_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a930ef1b",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "<class 'nibabel.nifti1.Nifti1Header'> object, endian='<'\n",
    "sizeof_hdr      : 348\n",
    "data_type       : b''\n",
    "db_name         : b''\n",
    "extents         : 0\n",
    "session_error   : 0\n",
    "regular         : b'r'\n",
    "dim_info        : 0\n",
    "dim             : [  3 128 128  70   1   1   1   1]\n",
    "intent_p1       : 0.0\n",
    "intent_p2       : 0.0\n",
    "intent_p3       : 0.0\n",
    "intent_code     : none\n",
    "datatype        : float32\n",
    "bitpix          : 32\n",
    "slice_start     : 0\n",
    "pixdim          : [-1.   2.   2.   2.2  0.   0.   0.   0. ]\n",
    "vox_offset      : 0.0\n",
    "scl_slope       : nan\n",
    "scl_inter       : nan\n",
    "slice_end       : 0\n",
    "slice_code      : unknown\n",
    "xyzt_units      : 10\n",
    "cal_max         : 0.0\n",
    "cal_min         : 0.0\n",
    "slice_duration  : 0.0\n",
    "toffset         : 0.0\n",
    "glmax           : 0\n",
    "glmin           : 0\n",
    "descrip         : b'5.0.11'\n",
    "aux_file        : b''\n",
    "qform_code      : scanner\n",
    "sform_code      : scanner\n",
    "quatern_b       : 0.0\n",
    "quatern_c       : 1.0\n",
    "quatern_d       : 0.0\n",
    "qoffset_x       : 125.5061\n",
    "qoffset_y       : -109.38977\n",
    "qoffset_z       : -86.742615\n",
    "srow_x          : [ -2.       0.       0.     125.5061]\n",
    "srow_y          : [   0.         2.         0.      -109.38977]\n",
    "srow_z          : [  0.         0.         2.2      -86.742615]\n",
    "intent_name     : b''\n",
    "magic           : b'n+1'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5033df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_info.get_xyzt_units())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f55222",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "('mm', 'sec')\n",
    "```\n",
    "\n",
    "With `get_fdata()` the image intensities can be converted to a Numpy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce31d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = img_nibabel.get_fdata()\n",
    "\n",
    "img1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a7a31d",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(128, 128, 70)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_nibabel = nib.load(\"data/b0_mask.nii\")\n",
    "\n",
    "img2 = img_nibabel.get_fdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d96cb31",
   "metadata": {},
   "source": [
    "We now import a second image with same spatial resolution, but additional information from another imaging modality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_nibabel = nib.load(\"data/b0_mask.nii\")\n",
    "\n",
    "img2 = img_nibabel.get_fdata()\n",
    "\n",
    "img2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f37d8a6",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(128, 128, 70)\n",
    "```\n",
    "\n",
    "For simpler handling, we only select a two-dimensional slice from each 3D image.\n",
    "\n",
    "### **Plot Images**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecce9019",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "img_slice = 30\n",
    "\n",
    "img1_slice = img1[:, :, img_slice]\n",
    "img2_slice = img2[:, :, img_slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import subplots, show\n",
    "\n",
    "fig, ax = subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "f1 = ax[0].imshow(img1_slice, cmap=\"gray\")\n",
    "f2 = ax[1].imshow(img2_slice, cmap=\"gray\")\n",
    "\n",
    "fig.colorbar(f1, ax=ax[0]);\n",
    "fig.colorbar(f2, ax=ax[1]);\n",
    "\n",
    "ax[0].set_xlabel('T1 Image', fontsize=16);\n",
    "ax[1].set_xlabel('B0 Image', fontsize=16);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cbabc9",
   "metadata": {},
   "source": [
    "<img src=\"fig/05-section5-rendered-unnamed-chunk-9-1.png\" width=\"1440\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "In order to check how similar the images are, we can compare their intensity histograms.\n",
    "\n",
    "### **Histograms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da787c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (img1_slice>0) & (img2_slice>0) \n",
    "\n",
    "img1_nz = img1_slice[mask]\n",
    "img2_nz = img2_slice[mask]\n",
    "\n",
    "fig, ax = subplots(nrows=1, ncols=2, figsize=(20, 5))\n",
    "\n",
    "ax[0].hist(img1_nz, bins=50);\n",
    "ax[1].hist(img2_nz, bins=50);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c3c956",
   "metadata": {},
   "source": [
    "<img src=\"fig/05-section5-rendered-unnamed-chunk-10-3.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "These two images appear to have different, potentially complementary information.\n",
    "\n",
    "### **Visualise and Concatenate**\n",
    "\n",
    "We can visualise the intensity data in a scatter plot, a two-dimensional histogram, and a kernel density contour plot from the Seaborn library. For the Seaborn library, [see documentation:](https://seaborn.pydata.org) \n",
    "\n",
    "Here is the [documentation for the kernel density plot:](https://seaborn.pydata.org/generated/seaborn.kdeplot.html)\n",
    "\n",
    "Here are the plots:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Scatter plot\n",
    "ax[0].scatter(img1_nz, img2_nz)\n",
    "\n",
    "# 2D Histogram\n",
    "ax[1].hist2d(img1_nz, img2_nz, bins=50, vmax=10);\n",
    "\n",
    "from seaborn import kdeplot\n",
    "\n",
    "# Density Plot\n",
    "kdeplot(x=img1_nz, y=img2_nz, ax=ax[2]);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb86ef63",
   "metadata": {},
   "source": [
    "<img src=\"fig/05-section5-rendered-unnamed-chunk-11-5.png\" width=\"1920\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "The scatter plot (left) shows a distorted, noisy distribution but too many valuess seem to be on top of each other. The 2D histogram (centre) shows two regions (yellow) with large counts. The kernel density plot (right) confirms two regions of high density. \n",
    "\n",
    "For Machine Learning, the data need to be in a single two-dimensional array. Rows denote pixels and (in our case) there are two columns for two images. Numpy function concatenate helps us to create the array in the correct form to apply machine learning algorithms. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import concatenate\n",
    "\n",
    "all_imgs = concatenate([img1_nz.reshape(-1,1), img2_nz.reshape(-1,1)], axis=1)\n",
    "\n",
    "all_imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221c939",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(4009, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427f898",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Clustering\n",
    "After the data preparation steps, we can proceed with our machine learning analysis.\n",
    "\n",
    "We want to segment the images into different types of tissues. It is not obvious how to do this, as the intensity values in the above histograms are distributed without clear segmentation of intensities. We will nonetheless attempt to cluster the images using a Gaussian Mixture Model (GMM) algorithm.\n",
    "\n",
    "The result of the clustering are predicted labels for each of the pixels (voxels in general), according to the tissue type which the algorithms assigns.\n",
    "\n",
    "Here is the workflow:\n",
    "\n",
    "1. Import the GMM class from the Python machine learning library __Scikit-learn__.\n",
    "\n",
    "2. Decide the number of clusters that we expect to see.\n",
    "\n",
    "3. Set a random seed to make the outcome reproducible.\n",
    "\n",
    "4. Instantiate the clustering algorithm.\n",
    "\n",
    "5. Use function `fit_predict` to fit the model to the data and obtain the labels for each pixel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace0e49",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "\n",
    "RANDOM_STATE = 12345\n",
    "\n",
    "gmm = GaussianMixture(n_components=n_components, \n",
    "                      random_state=RANDOM_STATE)\n",
    "\n",
    "all_img_labels = gmm.fit_predict(all_imgs)\n",
    "\n",
    "all_img_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8367c88e",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "2\n",
    "```\n",
    "\n",
    "We can now plot a scatter plot of the (predicted) labels to see their distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ec309",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(4, 4))\n",
    "\n",
    "ax.scatter(img1_nz, img2_nz, c=all_img_labels, s=100)\n",
    "\n",
    "ax.set_xlabel('Image 1', fontsize=16)\n",
    "ax.set_ylabel('Image 2', fontsize=16);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0816bc",
   "metadata": {},
   "source": [
    "<img src=\"fig/05-section5-rendered-unnamed-chunk-15-7.png\" width=\"384\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "The yellow and green dots seem to indicate \"objects\" in the image. The purple dots are the remaining \"other\" pixels.\n",
    "\n",
    "Now, we need to map the predicted labels from the \"intensity space\" back to the original image in its anatomical space. We find that the algorithm can pick up structures that can be identified as separate compartments in the image. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900087a1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "\n",
    "all_img_labels_mapped = zeros(img1_slice.shape)\n",
    "\n",
    "all_img_labels_mapped[mask] = all_img_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f970834",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(4, 4))\n",
    "\n",
    "ax.imshow(all_img_labels_mapped);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6738efd3",
   "metadata": {},
   "source": [
    "<img src=\"fig/05-section5-rendered-unnamed-chunk-17-9.png\" width=\"384\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "## Exercise\n",
    "Redo the above sequence of steps with different pre-assigned number of clusters and compare the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4010ac",
   "metadata": {},
   "source": [
    "- NiBabel is a package to access various neuroimaging file formats.\n",
    "- Object detection in neuro-images can be done using Gaussian Mixture Models (GMM) clustering.\n",
    "- The `fit_predict` function fits a model to data and returns predicted labels for each data point.\n",
    "\n",
    "[r-markdown]: https://rmarkdown.rstudio.com/"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
